I1125 15:30:29.861814 145709 caffe.cpp:185] Using GPUs 0
I1125 15:30:29.877890 145709 caffe.cpp:190] GPU 0: Tesla K20c
I1125 15:30:30.344166 145709 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 1000
snapshot: 2000
snapshot_prefix: "caffenet_train"
solver_mode: GPU
device_id: 0
net: "/home/GTL/jloy/catkin_ws/src/shore_follower/models/train_val_fast.prototxt"
I1125 15:30:30.351553 145709 solver.cpp:91] Creating training net from net file: /home/GTL/jloy/catkin_ws/src/shore_follower/models/train_val_fast.prototxt
I1125 15:30:30.357847 145709 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1125 15:30:30.357888 145709 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1125 15:30:30.358081 145709 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
    mean_file: "/home/GTL/jloy/catkin_ws/src/shore_follower/all_data/trial02/imagenet_mean_fast.binaryproto"
  }
  data_param {
    source: "/home/GTL/jloy/catkin_ws/src/shore_follower/all_data/trial02/followshore_train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "conv3"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1125 15:30:30.358260 145709 layer_factory.hpp:77] Creating layer data
I1125 15:30:30.359181 145709 net.cpp:106] Creating Layer data
I1125 15:30:30.359239 145709 net.cpp:411] data -> data
I1125 15:30:30.359287 145709 net.cpp:411] data -> label
I1125 15:30:30.359314 145709 data_transformer.cpp:25] Loading mean file from: /home/GTL/jloy/catkin_ws/src/shore_follower/all_data/trial02/imagenet_mean_fast.binaryproto
I1125 15:30:30.369333 145730 db_lmdb.cpp:38] Opened lmdb /home/GTL/jloy/catkin_ws/src/shore_follower/all_data/trial02/followshore_train_lmdb
I1125 15:30:30.414417 145709 data_layer.cpp:41] output data size: 256,3,32,32
I1125 15:30:30.426147 145709 net.cpp:150] Setting up data
I1125 15:30:30.426214 145709 net.cpp:157] Top shape: 256 3 32 32 (786432)
I1125 15:30:30.426226 145709 net.cpp:157] Top shape: 256 (256)
I1125 15:30:30.426231 145709 net.cpp:165] Memory required for data: 3146752
I1125 15:30:30.426246 145709 layer_factory.hpp:77] Creating layer conv1
I1125 15:30:30.426287 145709 net.cpp:106] Creating Layer conv1
I1125 15:30:30.426301 145709 net.cpp:454] conv1 <- data
I1125 15:30:30.426324 145709 net.cpp:411] conv1 -> conv1
I1125 15:30:30.428846 145709 net.cpp:150] Setting up conv1
I1125 15:30:30.428871 145709 net.cpp:157] Top shape: 256 96 6 6 (884736)
I1125 15:30:30.428876 145709 net.cpp:165] Memory required for data: 6685696
I1125 15:30:30.428900 145709 layer_factory.hpp:77] Creating layer relu1
I1125 15:30:30.428915 145709 net.cpp:106] Creating Layer relu1
I1125 15:30:30.428920 145709 net.cpp:454] relu1 <- conv1
I1125 15:30:30.428928 145709 net.cpp:397] relu1 -> conv1 (in-place)
I1125 15:30:30.428943 145709 net.cpp:150] Setting up relu1
I1125 15:30:30.428958 145709 net.cpp:157] Top shape: 256 96 6 6 (884736)
I1125 15:30:30.428962 145709 net.cpp:165] Memory required for data: 10224640
I1125 15:30:30.428967 145709 layer_factory.hpp:77] Creating layer pool1
I1125 15:30:30.428977 145709 net.cpp:106] Creating Layer pool1
I1125 15:30:30.428980 145709 net.cpp:454] pool1 <- conv1
I1125 15:30:30.428987 145709 net.cpp:411] pool1 -> pool1
I1125 15:30:30.429059 145709 net.cpp:150] Setting up pool1
I1125 15:30:30.429070 145709 net.cpp:157] Top shape: 256 96 3 3 (221184)
I1125 15:30:30.429075 145709 net.cpp:165] Memory required for data: 11109376
I1125 15:30:30.429078 145709 layer_factory.hpp:77] Creating layer norm1
I1125 15:30:30.429092 145709 net.cpp:106] Creating Layer norm1
I1125 15:30:30.429097 145709 net.cpp:454] norm1 <- pool1
I1125 15:30:30.429105 145709 net.cpp:411] norm1 -> norm1
I1125 15:30:30.429159 145709 net.cpp:150] Setting up norm1
I1125 15:30:30.429170 145709 net.cpp:157] Top shape: 256 96 3 3 (221184)
I1125 15:30:30.429174 145709 net.cpp:165] Memory required for data: 11994112
I1125 15:30:30.429178 145709 layer_factory.hpp:77] Creating layer conv2
I1125 15:30:30.429194 145709 net.cpp:106] Creating Layer conv2
I1125 15:30:30.429200 145709 net.cpp:454] conv2 <- norm1
I1125 15:30:30.429210 145709 net.cpp:411] conv2 -> conv2
I1125 15:30:30.442580 145709 net.cpp:150] Setting up conv2
I1125 15:30:30.442610 145709 net.cpp:157] Top shape: 256 256 3 3 (589824)
I1125 15:30:30.442615 145709 net.cpp:165] Memory required for data: 14353408
I1125 15:30:30.442628 145709 layer_factory.hpp:77] Creating layer relu2
I1125 15:30:30.442639 145709 net.cpp:106] Creating Layer relu2
I1125 15:30:30.442679 145709 net.cpp:454] relu2 <- conv2
I1125 15:30:30.442687 145709 net.cpp:397] relu2 -> conv2 (in-place)
I1125 15:30:30.442698 145709 net.cpp:150] Setting up relu2
I1125 15:30:30.442706 145709 net.cpp:157] Top shape: 256 256 3 3 (589824)
I1125 15:30:30.442710 145709 net.cpp:165] Memory required for data: 16712704
I1125 15:30:30.442714 145709 layer_factory.hpp:77] Creating layer pool2
I1125 15:30:30.442723 145709 net.cpp:106] Creating Layer pool2
I1125 15:30:30.442726 145709 net.cpp:454] pool2 <- conv2
I1125 15:30:30.442736 145709 net.cpp:411] pool2 -> pool2
I1125 15:30:30.442786 145709 net.cpp:150] Setting up pool2
I1125 15:30:30.442797 145709 net.cpp:157] Top shape: 256 256 1 1 (65536)
I1125 15:30:30.442801 145709 net.cpp:165] Memory required for data: 16974848
I1125 15:30:30.442806 145709 layer_factory.hpp:77] Creating layer norm2
I1125 15:30:30.442821 145709 net.cpp:106] Creating Layer norm2
I1125 15:30:30.442827 145709 net.cpp:454] norm2 <- pool2
I1125 15:30:30.442834 145709 net.cpp:411] norm2 -> norm2
I1125 15:30:30.442878 145709 net.cpp:150] Setting up norm2
I1125 15:30:30.442888 145709 net.cpp:157] Top shape: 256 256 1 1 (65536)
I1125 15:30:30.442893 145709 net.cpp:165] Memory required for data: 17236992
I1125 15:30:30.442896 145709 layer_factory.hpp:77] Creating layer conv3
I1125 15:30:30.442912 145709 net.cpp:106] Creating Layer conv3
I1125 15:30:30.442919 145709 net.cpp:454] conv3 <- norm2
I1125 15:30:30.442926 145709 net.cpp:411] conv3 -> conv3
I1125 15:30:30.481305 145709 net.cpp:150] Setting up conv3
I1125 15:30:30.481326 145709 net.cpp:157] Top shape: 256 384 1 1 (98304)
I1125 15:30:30.481331 145709 net.cpp:165] Memory required for data: 17630208
I1125 15:30:30.481344 145709 layer_factory.hpp:77] Creating layer relu3
I1125 15:30:30.481358 145709 net.cpp:106] Creating Layer relu3
I1125 15:30:30.481362 145709 net.cpp:454] relu3 <- conv3
I1125 15:30:30.481371 145709 net.cpp:397] relu3 -> conv3 (in-place)
I1125 15:30:30.481380 145709 net.cpp:150] Setting up relu3
I1125 15:30:30.481389 145709 net.cpp:157] Top shape: 256 384 1 1 (98304)
I1125 15:30:30.481393 145709 net.cpp:165] Memory required for data: 18023424
I1125 15:30:30.481397 145709 layer_factory.hpp:77] Creating layer fc6
I1125 15:30:30.481413 145709 net.cpp:106] Creating Layer fc6
I1125 15:30:30.481420 145709 net.cpp:454] fc6 <- conv3
I1125 15:30:30.481428 145709 net.cpp:411] fc6 -> fc6
I1125 15:30:30.548848 145709 net.cpp:150] Setting up fc6
I1125 15:30:30.548869 145709 net.cpp:157] Top shape: 256 4096 (1048576)
I1125 15:30:30.548874 145709 net.cpp:165] Memory required for data: 22217728
I1125 15:30:30.548884 145709 layer_factory.hpp:77] Creating layer relu6
I1125 15:30:30.548893 145709 net.cpp:106] Creating Layer relu6
I1125 15:30:30.548898 145709 net.cpp:454] relu6 <- fc6
I1125 15:30:30.548909 145709 net.cpp:397] relu6 -> fc6 (in-place)
I1125 15:30:30.548918 145709 net.cpp:150] Setting up relu6
I1125 15:30:30.548928 145709 net.cpp:157] Top shape: 256 4096 (1048576)
I1125 15:30:30.548933 145709 net.cpp:165] Memory required for data: 26412032
I1125 15:30:30.548936 145709 layer_factory.hpp:77] Creating layer drop6
I1125 15:30:30.548949 145709 net.cpp:106] Creating Layer drop6
I1125 15:30:30.548956 145709 net.cpp:454] drop6 <- fc6
I1125 15:30:30.548965 145709 net.cpp:397] drop6 -> fc6 (in-place)
I1125 15:30:30.548998 145709 net.cpp:150] Setting up drop6
I1125 15:30:30.549007 145709 net.cpp:157] Top shape: 256 4096 (1048576)
I1125 15:30:30.549011 145709 net.cpp:165] Memory required for data: 30606336
I1125 15:30:30.549016 145709 layer_factory.hpp:77] Creating layer fc7
I1125 15:30:30.549026 145709 net.cpp:106] Creating Layer fc7
I1125 15:30:30.549029 145709 net.cpp:454] fc7 <- fc6
I1125 15:30:30.549038 145709 net.cpp:411] fc7 -> fc7
I1125 15:30:31.271574 145709 net.cpp:150] Setting up fc7
I1125 15:30:31.271615 145709 net.cpp:157] Top shape: 256 4096 (1048576)
I1125 15:30:31.271621 145709 net.cpp:165] Memory required for data: 34800640
I1125 15:30:31.271646 145709 layer_factory.hpp:77] Creating layer relu7
I1125 15:30:31.271661 145709 net.cpp:106] Creating Layer relu7
I1125 15:30:31.271708 145709 net.cpp:454] relu7 <- fc7
I1125 15:30:31.271718 145709 net.cpp:397] relu7 -> fc7 (in-place)
I1125 15:30:31.271730 145709 net.cpp:150] Setting up relu7
I1125 15:30:31.271739 145709 net.cpp:157] Top shape: 256 4096 (1048576)
I1125 15:30:31.271742 145709 net.cpp:165] Memory required for data: 38994944
I1125 15:30:31.271747 145709 layer_factory.hpp:77] Creating layer drop7
I1125 15:30:31.271764 145709 net.cpp:106] Creating Layer drop7
I1125 15:30:31.271770 145709 net.cpp:454] drop7 <- fc7
I1125 15:30:31.271775 145709 net.cpp:397] drop7 -> fc7 (in-place)
I1125 15:30:31.271805 145709 net.cpp:150] Setting up drop7
I1125 15:30:31.271814 145709 net.cpp:157] Top shape: 256 4096 (1048576)
I1125 15:30:31.271818 145709 net.cpp:165] Memory required for data: 43189248
I1125 15:30:31.271822 145709 layer_factory.hpp:77] Creating layer fc8
I1125 15:30:31.271836 145709 net.cpp:106] Creating Layer fc8
I1125 15:30:31.271842 145709 net.cpp:454] fc8 <- fc7
I1125 15:30:31.271850 145709 net.cpp:411] fc8 -> fc8
I1125 15:30:31.273350 145709 net.cpp:150] Setting up fc8
I1125 15:30:31.273368 145709 net.cpp:157] Top shape: 256 3 (768)
I1125 15:30:31.273372 145709 net.cpp:165] Memory required for data: 43192320
I1125 15:30:31.273381 145709 layer_factory.hpp:77] Creating layer loss
I1125 15:30:31.273398 145709 net.cpp:106] Creating Layer loss
I1125 15:30:31.273403 145709 net.cpp:454] loss <- fc8
I1125 15:30:31.273408 145709 net.cpp:454] loss <- label
I1125 15:30:31.273422 145709 net.cpp:411] loss -> loss
I1125 15:30:31.273442 145709 layer_factory.hpp:77] Creating layer loss
I1125 15:30:31.273560 145709 net.cpp:150] Setting up loss
I1125 15:30:31.273571 145709 net.cpp:157] Top shape: (1)
I1125 15:30:31.273574 145709 net.cpp:160]     with loss weight 1
I1125 15:30:31.273610 145709 net.cpp:165] Memory required for data: 43192324
I1125 15:30:31.273615 145709 net.cpp:226] loss needs backward computation.
I1125 15:30:31.273620 145709 net.cpp:226] fc8 needs backward computation.
I1125 15:30:31.273624 145709 net.cpp:226] drop7 needs backward computation.
I1125 15:30:31.273628 145709 net.cpp:226] relu7 needs backward computation.
I1125 15:30:31.273632 145709 net.cpp:226] fc7 needs backward computation.
I1125 15:30:31.273635 145709 net.cpp:226] drop6 needs backward computation.
I1125 15:30:31.273639 145709 net.cpp:226] relu6 needs backward computation.
I1125 15:30:31.273643 145709 net.cpp:226] fc6 needs backward computation.
I1125 15:30:31.273646 145709 net.cpp:226] relu3 needs backward computation.
I1125 15:30:31.273650 145709 net.cpp:226] conv3 needs backward computation.
I1125 15:30:31.273654 145709 net.cpp:226] norm2 needs backward computation.
I1125 15:30:31.273658 145709 net.cpp:226] pool2 needs backward computation.
I1125 15:30:31.273663 145709 net.cpp:226] relu2 needs backward computation.
I1125 15:30:31.273666 145709 net.cpp:226] conv2 needs backward computation.
I1125 15:30:31.273670 145709 net.cpp:226] norm1 needs backward computation.
I1125 15:30:31.273674 145709 net.cpp:226] pool1 needs backward computation.
I1125 15:30:31.273679 145709 net.cpp:226] relu1 needs backward computation.
I1125 15:30:31.273682 145709 net.cpp:226] conv1 needs backward computation.
I1125 15:30:31.273686 145709 net.cpp:228] data does not need backward computation.
I1125 15:30:31.273690 145709 net.cpp:270] This network produces output loss
I1125 15:30:31.273707 145709 net.cpp:283] Network initialization done.
I1125 15:30:31.278220 145709 solver.cpp:181] Creating test net (#0) specified by net file: /home/GTL/jloy/catkin_ws/src/shore_follower/models/train_val_fast.prototxt
I1125 15:30:31.278283 145709 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1125 15:30:31.278487 145709 net.cpp:49] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 32
    mean_file: "/home/GTL/jloy/catkin_ws/src/shore_follower/all_data/trial02/imagenet_mean_fast.binaryproto"
  }
  data_param {
    source: "/home/GTL/jloy/catkin_ws/src/shore_follower/all_data/trial02/followshore_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "conv3"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1125 15:30:31.278656 145709 layer_factory.hpp:77] Creating layer data
I1125 15:30:31.278771 145709 net.cpp:106] Creating Layer data
I1125 15:30:31.278784 145709 net.cpp:411] data -> data
I1125 15:30:31.278796 145709 net.cpp:411] data -> label
I1125 15:30:31.278807 145709 data_transformer.cpp:25] Loading mean file from: /home/GTL/jloy/catkin_ws/src/shore_follower/all_data/trial02/imagenet_mean_fast.binaryproto
I1125 15:30:31.289500 145748 db_lmdb.cpp:38] Opened lmdb /home/GTL/jloy/catkin_ws/src/shore_follower/all_data/trial02/followshore_val_lmdb
I1125 15:30:31.306495 145709 data_layer.cpp:41] output data size: 50,3,32,32
I1125 15:30:31.312568 145709 net.cpp:150] Setting up data
I1125 15:30:31.312592 145709 net.cpp:157] Top shape: 50 3 32 32 (153600)
I1125 15:30:31.312598 145709 net.cpp:157] Top shape: 50 (50)
I1125 15:30:31.312602 145709 net.cpp:165] Memory required for data: 614600
I1125 15:30:31.312609 145709 layer_factory.hpp:77] Creating layer label_data_1_split
I1125 15:30:31.312623 145709 net.cpp:106] Creating Layer label_data_1_split
I1125 15:30:31.312628 145709 net.cpp:454] label_data_1_split <- label
I1125 15:30:31.312634 145709 net.cpp:411] label_data_1_split -> label_data_1_split_0
I1125 15:30:31.312644 145709 net.cpp:411] label_data_1_split -> label_data_1_split_1
I1125 15:30:31.312753 145709 net.cpp:150] Setting up label_data_1_split
I1125 15:30:31.312767 145709 net.cpp:157] Top shape: 50 (50)
I1125 15:30:31.312773 145709 net.cpp:157] Top shape: 50 (50)
I1125 15:30:31.312777 145709 net.cpp:165] Memory required for data: 615000
I1125 15:30:31.312782 145709 layer_factory.hpp:77] Creating layer conv1
I1125 15:30:31.312798 145709 net.cpp:106] Creating Layer conv1
I1125 15:30:31.312804 145709 net.cpp:454] conv1 <- data
I1125 15:30:31.312814 145709 net.cpp:411] conv1 -> conv1
I1125 15:30:31.314522 145709 net.cpp:150] Setting up conv1
I1125 15:30:31.314537 145709 net.cpp:157] Top shape: 50 96 6 6 (172800)
I1125 15:30:31.314541 145709 net.cpp:165] Memory required for data: 1306200
I1125 15:30:31.314554 145709 layer_factory.hpp:77] Creating layer relu1
I1125 15:30:31.314565 145709 net.cpp:106] Creating Layer relu1
I1125 15:30:31.314569 145709 net.cpp:454] relu1 <- conv1
I1125 15:30:31.314575 145709 net.cpp:397] relu1 -> conv1 (in-place)
I1125 15:30:31.314584 145709 net.cpp:150] Setting up relu1
I1125 15:30:31.314589 145709 net.cpp:157] Top shape: 50 96 6 6 (172800)
I1125 15:30:31.314594 145709 net.cpp:165] Memory required for data: 1997400
I1125 15:30:31.314597 145709 layer_factory.hpp:77] Creating layer pool1
I1125 15:30:31.314605 145709 net.cpp:106] Creating Layer pool1
I1125 15:30:31.314610 145709 net.cpp:454] pool1 <- conv1
I1125 15:30:31.314622 145709 net.cpp:411] pool1 -> pool1
I1125 15:30:31.314676 145709 net.cpp:150] Setting up pool1
I1125 15:30:31.314687 145709 net.cpp:157] Top shape: 50 96 3 3 (43200)
I1125 15:30:31.314692 145709 net.cpp:165] Memory required for data: 2170200
I1125 15:30:31.314697 145709 layer_factory.hpp:77] Creating layer norm1
I1125 15:30:31.314707 145709 net.cpp:106] Creating Layer norm1
I1125 15:30:31.314710 145709 net.cpp:454] norm1 <- pool1
I1125 15:30:31.314719 145709 net.cpp:411] norm1 -> norm1
I1125 15:30:31.314761 145709 net.cpp:150] Setting up norm1
I1125 15:30:31.314770 145709 net.cpp:157] Top shape: 50 96 3 3 (43200)
I1125 15:30:31.314774 145709 net.cpp:165] Memory required for data: 2343000
I1125 15:30:31.314779 145709 layer_factory.hpp:77] Creating layer conv2
I1125 15:30:31.314790 145709 net.cpp:106] Creating Layer conv2
I1125 15:30:31.314795 145709 net.cpp:454] conv2 <- norm1
I1125 15:30:31.314805 145709 net.cpp:411] conv2 -> conv2
I1125 15:30:31.328222 145709 net.cpp:150] Setting up conv2
I1125 15:30:31.328241 145709 net.cpp:157] Top shape: 50 256 3 3 (115200)
I1125 15:30:31.328246 145709 net.cpp:165] Memory required for data: 2803800
I1125 15:30:31.328258 145709 layer_factory.hpp:77] Creating layer relu2
I1125 15:30:31.328268 145709 net.cpp:106] Creating Layer relu2
I1125 15:30:31.328271 145709 net.cpp:454] relu2 <- conv2
I1125 15:30:31.328280 145709 net.cpp:397] relu2 -> conv2 (in-place)
I1125 15:30:31.328289 145709 net.cpp:150] Setting up relu2
I1125 15:30:31.328295 145709 net.cpp:157] Top shape: 50 256 3 3 (115200)
I1125 15:30:31.328299 145709 net.cpp:165] Memory required for data: 3264600
I1125 15:30:31.328303 145709 layer_factory.hpp:77] Creating layer pool2
I1125 15:30:31.328330 145709 net.cpp:106] Creating Layer pool2
I1125 15:30:31.328342 145709 net.cpp:454] pool2 <- conv2
I1125 15:30:31.328352 145709 net.cpp:411] pool2 -> pool2
I1125 15:30:31.328402 145709 net.cpp:150] Setting up pool2
I1125 15:30:31.328415 145709 net.cpp:157] Top shape: 50 256 1 1 (12800)
I1125 15:30:31.328419 145709 net.cpp:165] Memory required for data: 3315800
I1125 15:30:31.328423 145709 layer_factory.hpp:77] Creating layer norm2
I1125 15:30:31.328430 145709 net.cpp:106] Creating Layer norm2
I1125 15:30:31.328434 145709 net.cpp:454] norm2 <- pool2
I1125 15:30:31.328440 145709 net.cpp:411] norm2 -> norm2
I1125 15:30:31.328483 145709 net.cpp:150] Setting up norm2
I1125 15:30:31.328493 145709 net.cpp:157] Top shape: 50 256 1 1 (12800)
I1125 15:30:31.328496 145709 net.cpp:165] Memory required for data: 3367000
I1125 15:30:31.328500 145709 layer_factory.hpp:77] Creating layer conv3
I1125 15:30:31.328516 145709 net.cpp:106] Creating Layer conv3
I1125 15:30:31.328521 145709 net.cpp:454] conv3 <- norm2
I1125 15:30:31.328531 145709 net.cpp:411] conv3 -> conv3
I1125 15:30:31.366164 145709 net.cpp:150] Setting up conv3
I1125 15:30:31.366183 145709 net.cpp:157] Top shape: 50 384 1 1 (19200)
I1125 15:30:31.366189 145709 net.cpp:165] Memory required for data: 3443800
I1125 15:30:31.366200 145709 layer_factory.hpp:77] Creating layer relu3
I1125 15:30:31.366209 145709 net.cpp:106] Creating Layer relu3
I1125 15:30:31.366214 145709 net.cpp:454] relu3 <- conv3
I1125 15:30:31.366220 145709 net.cpp:397] relu3 -> conv3 (in-place)
I1125 15:30:31.366228 145709 net.cpp:150] Setting up relu3
I1125 15:30:31.366235 145709 net.cpp:157] Top shape: 50 384 1 1 (19200)
I1125 15:30:31.366238 145709 net.cpp:165] Memory required for data: 3520600
I1125 15:30:31.366242 145709 layer_factory.hpp:77] Creating layer fc6
I1125 15:30:31.366253 145709 net.cpp:106] Creating Layer fc6
I1125 15:30:31.366258 145709 net.cpp:454] fc6 <- conv3
I1125 15:30:31.366264 145709 net.cpp:411] fc6 -> fc6
I1125 15:30:31.436156 145709 net.cpp:150] Setting up fc6
I1125 15:30:31.436188 145709 net.cpp:157] Top shape: 50 4096 (204800)
I1125 15:30:31.436194 145709 net.cpp:165] Memory required for data: 4339800
I1125 15:30:31.436208 145709 layer_factory.hpp:77] Creating layer relu6
I1125 15:30:31.436219 145709 net.cpp:106] Creating Layer relu6
I1125 15:30:31.436228 145709 net.cpp:454] relu6 <- fc6
I1125 15:30:31.436234 145709 net.cpp:397] relu6 -> fc6 (in-place)
I1125 15:30:31.436244 145709 net.cpp:150] Setting up relu6
I1125 15:30:31.436249 145709 net.cpp:157] Top shape: 50 4096 (204800)
I1125 15:30:31.436254 145709 net.cpp:165] Memory required for data: 5159000
I1125 15:30:31.436259 145709 layer_factory.hpp:77] Creating layer drop6
I1125 15:30:31.436269 145709 net.cpp:106] Creating Layer drop6
I1125 15:30:31.436274 145709 net.cpp:454] drop6 <- fc6
I1125 15:30:31.436281 145709 net.cpp:397] drop6 -> fc6 (in-place)
I1125 15:30:31.436328 145709 net.cpp:150] Setting up drop6
I1125 15:30:31.436342 145709 net.cpp:157] Top shape: 50 4096 (204800)
I1125 15:30:31.436345 145709 net.cpp:165] Memory required for data: 5978200
I1125 15:30:31.436350 145709 layer_factory.hpp:77] Creating layer fc7
I1125 15:30:31.436359 145709 net.cpp:106] Creating Layer fc7
I1125 15:30:31.436363 145709 net.cpp:454] fc7 <- fc6
I1125 15:30:31.436375 145709 net.cpp:411] fc7 -> fc7
I1125 15:30:32.174608 145709 net.cpp:150] Setting up fc7
I1125 15:30:32.174654 145709 net.cpp:157] Top shape: 50 4096 (204800)
I1125 15:30:32.174660 145709 net.cpp:165] Memory required for data: 6797400
I1125 15:30:32.174685 145709 layer_factory.hpp:77] Creating layer relu7
I1125 15:30:32.174711 145709 net.cpp:106] Creating Layer relu7
I1125 15:30:32.174718 145709 net.cpp:454] relu7 <- fc7
I1125 15:30:32.174727 145709 net.cpp:397] relu7 -> fc7 (in-place)
I1125 15:30:32.174741 145709 net.cpp:150] Setting up relu7
I1125 15:30:32.174747 145709 net.cpp:157] Top shape: 50 4096 (204800)
I1125 15:30:32.174751 145709 net.cpp:165] Memory required for data: 7616600
I1125 15:30:32.174756 145709 layer_factory.hpp:77] Creating layer drop7
I1125 15:30:32.174804 145709 net.cpp:106] Creating Layer drop7
I1125 15:30:32.174813 145709 net.cpp:454] drop7 <- fc7
I1125 15:30:32.174823 145709 net.cpp:397] drop7 -> fc7 (in-place)
I1125 15:30:32.174861 145709 net.cpp:150] Setting up drop7
I1125 15:30:32.174871 145709 net.cpp:157] Top shape: 50 4096 (204800)
I1125 15:30:32.174875 145709 net.cpp:165] Memory required for data: 8435800
I1125 15:30:32.174880 145709 layer_factory.hpp:77] Creating layer fc8
I1125 15:30:32.174893 145709 net.cpp:106] Creating Layer fc8
I1125 15:30:32.174897 145709 net.cpp:454] fc8 <- fc7
I1125 15:30:32.174904 145709 net.cpp:411] fc8 -> fc8
I1125 15:30:32.175575 145709 net.cpp:150] Setting up fc8
I1125 15:30:32.175587 145709 net.cpp:157] Top shape: 50 3 (150)
I1125 15:30:32.175591 145709 net.cpp:165] Memory required for data: 8436400
I1125 15:30:32.175600 145709 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1125 15:30:32.175608 145709 net.cpp:106] Creating Layer fc8_fc8_0_split
I1125 15:30:32.175613 145709 net.cpp:454] fc8_fc8_0_split <- fc8
I1125 15:30:32.175623 145709 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1125 15:30:32.175631 145709 net.cpp:411] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1125 15:30:32.175678 145709 net.cpp:150] Setting up fc8_fc8_0_split
I1125 15:30:32.175688 145709 net.cpp:157] Top shape: 50 3 (150)
I1125 15:30:32.175693 145709 net.cpp:157] Top shape: 50 3 (150)
I1125 15:30:32.175698 145709 net.cpp:165] Memory required for data: 8437600
I1125 15:30:32.175701 145709 layer_factory.hpp:77] Creating layer accuracy
I1125 15:30:32.175721 145709 net.cpp:106] Creating Layer accuracy
I1125 15:30:32.175727 145709 net.cpp:454] accuracy <- fc8_fc8_0_split_0
I1125 15:30:32.175734 145709 net.cpp:454] accuracy <- label_data_1_split_0
I1125 15:30:32.175743 145709 net.cpp:411] accuracy -> accuracy
I1125 15:30:32.175761 145709 net.cpp:150] Setting up accuracy
I1125 15:30:32.175768 145709 net.cpp:157] Top shape: (1)
I1125 15:30:32.175776 145709 net.cpp:165] Memory required for data: 8437604
I1125 15:30:32.175781 145709 layer_factory.hpp:77] Creating layer loss
I1125 15:30:32.175788 145709 net.cpp:106] Creating Layer loss
I1125 15:30:32.175792 145709 net.cpp:454] loss <- fc8_fc8_0_split_1
I1125 15:30:32.175797 145709 net.cpp:454] loss <- label_data_1_split_1
I1125 15:30:32.175808 145709 net.cpp:411] loss -> loss
I1125 15:30:32.175820 145709 layer_factory.hpp:77] Creating layer loss
I1125 15:30:32.175935 145709 net.cpp:150] Setting up loss
I1125 15:30:32.175946 145709 net.cpp:157] Top shape: (1)
I1125 15:30:32.175951 145709 net.cpp:160]     with loss weight 1
I1125 15:30:32.175972 145709 net.cpp:165] Memory required for data: 8437608
I1125 15:30:32.175977 145709 net.cpp:226] loss needs backward computation.
I1125 15:30:32.175982 145709 net.cpp:228] accuracy does not need backward computation.
I1125 15:30:32.175987 145709 net.cpp:226] fc8_fc8_0_split needs backward computation.
I1125 15:30:32.175992 145709 net.cpp:226] fc8 needs backward computation.
I1125 15:30:32.175995 145709 net.cpp:226] drop7 needs backward computation.
I1125 15:30:32.175998 145709 net.cpp:226] relu7 needs backward computation.
I1125 15:30:32.176002 145709 net.cpp:226] fc7 needs backward computation.
I1125 15:30:32.176007 145709 net.cpp:226] drop6 needs backward computation.
I1125 15:30:32.176009 145709 net.cpp:226] relu6 needs backward computation.
I1125 15:30:32.176013 145709 net.cpp:226] fc6 needs backward computation.
I1125 15:30:32.176017 145709 net.cpp:226] relu3 needs backward computation.
I1125 15:30:32.176021 145709 net.cpp:226] conv3 needs backward computation.
I1125 15:30:32.176025 145709 net.cpp:226] norm2 needs backward computation.
I1125 15:30:32.176029 145709 net.cpp:226] pool2 needs backward computation.
I1125 15:30:32.176033 145709 net.cpp:226] relu2 needs backward computation.
I1125 15:30:32.176036 145709 net.cpp:226] conv2 needs backward computation.
I1125 15:30:32.176040 145709 net.cpp:226] norm1 needs backward computation.
I1125 15:30:32.176044 145709 net.cpp:226] pool1 needs backward computation.
I1125 15:30:32.176065 145709 net.cpp:226] relu1 needs backward computation.
I1125 15:30:32.176070 145709 net.cpp:226] conv1 needs backward computation.
I1125 15:30:32.176074 145709 net.cpp:228] label_data_1_split does not need backward computation.
I1125 15:30:32.176079 145709 net.cpp:228] data does not need backward computation.
I1125 15:30:32.176082 145709 net.cpp:270] This network produces output accuracy
I1125 15:30:32.176087 145709 net.cpp:270] This network produces output loss
I1125 15:30:32.176108 145709 net.cpp:283] Network initialization done.
I1125 15:30:32.176241 145709 solver.cpp:60] Solver scaffolding done.
I1125 15:30:32.176743 145709 caffe.cpp:219] Starting Optimization
I1125 15:30:32.176754 145709 solver.cpp:280] Solving CaffeNet
I1125 15:30:32.176759 145709 solver.cpp:281] Learning Rate Policy: step
I1125 15:30:32.178851 145709 solver.cpp:338] Iteration 0, Testing net (#0)
I1125 15:32:06.446667 145709 solver.cpp:406]     Test net output #0: accuracy = 0.321
I1125 15:32:06.446790 145709 solver.cpp:406]     Test net output #1: loss = 1.1107 (* 1 = 1.1107 loss)
I1125 15:32:07.101158 145709 solver.cpp:229] Iteration 0, loss = 1.27938
I1125 15:32:07.101191 145709 solver.cpp:245]     Train net output #0: loss = 1.27938 (* 1 = 1.27938 loss)
I1125 15:32:07.101217 145709 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1125 15:33:21.075516 145709 solver.cpp:229] Iteration 100, loss = 1.09833
I1125 15:33:21.075651 145709 solver.cpp:245]     Train net output #0: loss = 1.09833 (* 1 = 1.09833 loss)
I1125 15:33:21.075664 145709 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I1125 15:34:34.976871 145709 solver.cpp:229] Iteration 200, loss = 1.0958
I1125 15:34:34.977061 145709 solver.cpp:245]     Train net output #0: loss = 1.0958 (* 1 = 1.0958 loss)
I1125 15:34:34.977083 145709 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I1125 15:35:48.910300 145709 solver.cpp:229] Iteration 300, loss = 1.09921
I1125 15:35:48.910434 145709 solver.cpp:245]     Train net output #0: loss = 1.09921 (* 1 = 1.09921 loss)
I1125 15:35:48.910444 145709 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I1125 15:37:02.870728 145709 solver.cpp:229] Iteration 400, loss = 1.09908
I1125 15:37:02.870862 145709 solver.cpp:245]     Train net output #0: loss = 1.09908 (* 1 = 1.09908 loss)
I1125 15:37:02.870873 145709 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I1125 15:38:16.821836 145709 solver.cpp:229] Iteration 500, loss = 1.09625
I1125 15:38:16.821969 145709 solver.cpp:245]     Train net output #0: loss = 1.09625 (* 1 = 1.09625 loss)
I1125 15:38:16.821979 145709 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I1125 15:39:30.785246 145709 solver.cpp:229] Iteration 600, loss = 0.837269
I1125 15:39:30.785373 145709 solver.cpp:245]     Train net output #0: loss = 0.837269 (* 1 = 0.837269 loss)
I1125 15:39:30.785383 145709 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I1125 15:40:37.617612 145709 solver.cpp:229] Iteration 700, loss = 0.726215
I1125 15:40:37.617738 145709 solver.cpp:245]     Train net output #0: loss = 0.726214 (* 1 = 0.726214 loss)
I1125 15:40:37.617748 145709 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I1125 15:41:46.562011 145709 solver.cpp:229] Iteration 800, loss = 0.647827
I1125 15:41:46.562137 145709 solver.cpp:245]     Train net output #0: loss = 0.647826 (* 1 = 0.647826 loss)
I1125 15:41:46.562146 145709 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I1125 15:43:00.555879 145709 solver.cpp:229] Iteration 900, loss = 0.632788
I1125 15:43:00.556066 145709 solver.cpp:245]     Train net output #0: loss = 0.632788 (* 1 = 0.632788 loss)
I1125 15:43:00.556090 145709 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I1125 15:44:13.793131 145709 solver.cpp:338] Iteration 1000, Testing net (#0)
I1125 15:45:48.129701 145709 solver.cpp:406]     Test net output #0: accuracy = 0.653
I1125 15:45:48.129832 145709 solver.cpp:406]     Test net output #1: loss = 1.02261 (* 1 = 1.02261 loss)
I1125 15:45:48.765724 145709 solver.cpp:229] Iteration 1000, loss = 0.477387
I1125 15:45:48.765759 145709 solver.cpp:245]     Train net output #0: loss = 0.477387 (* 1 = 0.477387 loss)
I1125 15:45:48.765772 145709 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I1125 15:47:02.694638 145709 solver.cpp:229] Iteration 1100, loss = 0.306177
I1125 15:47:02.694818 145709 solver.cpp:245]     Train net output #0: loss = 0.306176 (* 1 = 0.306176 loss)
I1125 15:47:02.694829 145709 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I1125 15:48:16.629456 145709 solver.cpp:229] Iteration 1200, loss = 0.197011
I1125 15:48:16.629645 145709 solver.cpp:245]     Train net output #0: loss = 0.19701 (* 1 = 0.19701 loss)
I1125 15:48:16.629669 145709 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I1125 15:49:30.585417 145709 solver.cpp:229] Iteration 1300, loss = 0.157091
I1125 15:49:30.585618 145709 solver.cpp:245]     Train net output #0: loss = 0.15709 (* 1 = 0.15709 loss)
I1125 15:49:30.585642 145709 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I1125 15:50:44.534731 145709 solver.cpp:229] Iteration 1400, loss = 0.128826
I1125 15:50:44.534869 145709 solver.cpp:245]     Train net output #0: loss = 0.128825 (* 1 = 0.128825 loss)
I1125 15:50:44.534880 145709 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I1125 15:51:58.499037 145709 solver.cpp:229] Iteration 1500, loss = 0.165405
I1125 15:51:58.499176 145709 solver.cpp:245]     Train net output #0: loss = 0.165405 (* 1 = 0.165405 loss)
I1125 15:51:58.499187 145709 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I1125 15:53:12.472187 145709 solver.cpp:229] Iteration 1600, loss = 0.113181
I1125 15:53:12.472321 145709 solver.cpp:245]     Train net output #0: loss = 0.11318 (* 1 = 0.11318 loss)
I1125 15:53:12.472332 145709 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I1125 15:54:21.601588 145709 solver.cpp:229] Iteration 1700, loss = 0.0889532
I1125 15:54:21.601728 145709 solver.cpp:245]     Train net output #0: loss = 0.0889525 (* 1 = 0.0889525 loss)
I1125 15:54:21.601739 145709 sgd_solver.cpp:106] Iteration 1700, lr = 0.001
I1125 15:55:30.532055 145709 solver.cpp:229] Iteration 1800, loss = 0.109064
I1125 15:55:30.532191 145709 solver.cpp:245]     Train net output #0: loss = 0.109063 (* 1 = 0.109063 loss)
I1125 15:55:30.532201 145709 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I1125 15:56:44.533660 145709 solver.cpp:229] Iteration 1900, loss = 0.131368
I1125 15:56:44.533803 145709 solver.cpp:245]     Train net output #0: loss = 0.131368 (* 1 = 0.131368 loss)
I1125 15:56:44.533815 145709 sgd_solver.cpp:106] Iteration 1900, lr = 0.001
I1125 15:57:57.788867 145709 solver.cpp:456] Snapshotting to binary proto file caffenet_train_iter_2000.caffemodel
I1125 15:57:59.130296 145709 sgd_solver.cpp:273] Snapshotting solver state to binary proto file caffenet_train_iter_2000.solverstate
I1125 15:58:00.076493 145709 solver.cpp:338] Iteration 2000, Testing net (#0)
I1125 15:59:34.297196 145709 solver.cpp:406]     Test net output #0: accuracy = 0.612001
I1125 15:59:34.297346 145709 solver.cpp:406]     Test net output #1: loss = 3.07706 (* 1 = 3.07706 loss)
I1125 15:59:34.935822 145709 solver.cpp:229] Iteration 2000, loss = 0.101822
I1125 15:59:34.935860 145709 solver.cpp:245]     Train net output #0: loss = 0.101821 (* 1 = 0.101821 loss)
I1125 15:59:34.935874 145709 sgd_solver.cpp:106] Iteration 2000, lr = 0.0001
I1125 16:00:48.923221 145709 solver.cpp:229] Iteration 2100, loss = 0.0932376
I1125 16:00:48.923364 145709 solver.cpp:245]     Train net output #0: loss = 0.0932369 (* 1 = 0.0932369 loss)
I1125 16:00:48.923377 145709 sgd_solver.cpp:106] Iteration 2100, lr = 0.0001
I1125 16:02:02.848783 145709 solver.cpp:229] Iteration 2200, loss = 0.0512097
I1125 16:02:02.848924 145709 solver.cpp:245]     Train net output #0: loss = 0.051209 (* 1 = 0.051209 loss)
I1125 16:02:02.848937 145709 sgd_solver.cpp:106] Iteration 2200, lr = 0.0001
I1125 16:03:16.812831 145709 solver.cpp:229] Iteration 2300, loss = 0.10729
I1125 16:03:16.812973 145709 solver.cpp:245]     Train net output #0: loss = 0.10729 (* 1 = 0.10729 loss)
I1125 16:03:16.812985 145709 sgd_solver.cpp:106] Iteration 2300, lr = 0.0001
I1125 16:04:30.777765 145709 solver.cpp:229] Iteration 2400, loss = 0.075063
I1125 16:04:30.777993 145709 solver.cpp:245]     Train net output #0: loss = 0.0750623 (* 1 = 0.0750623 loss)
I1125 16:04:30.778018 145709 sgd_solver.cpp:106] Iteration 2400, lr = 0.0001
I1125 16:05:44.733834 145709 solver.cpp:229] Iteration 2500, loss = 0.102849
I1125 16:05:44.734031 145709 solver.cpp:245]     Train net output #0: loss = 0.102849 (* 1 = 0.102849 loss)
I1125 16:05:44.734056 145709 sgd_solver.cpp:106] Iteration 2500, lr = 0.0001
I1125 16:06:58.684108 145709 solver.cpp:229] Iteration 2600, loss = 0.0721732
I1125 16:06:58.684247 145709 solver.cpp:245]     Train net output #0: loss = 0.0721725 (* 1 = 0.0721725 loss)
I1125 16:06:58.684259 145709 sgd_solver.cpp:106] Iteration 2600, lr = 0.0001
I1125 16:08:05.372313 145709 solver.cpp:229] Iteration 2700, loss = 0.0525442
I1125 16:08:05.372483 145709 solver.cpp:245]     Train net output #0: loss = 0.0525435 (* 1 = 0.0525435 loss)
I1125 16:08:05.372509 145709 sgd_solver.cpp:106] Iteration 2700, lr = 0.0001
I1125 16:09:14.273834 145709 solver.cpp:229] Iteration 2800, loss = 0.0853084
I1125 16:09:14.274034 145709 solver.cpp:245]     Train net output #0: loss = 0.0853077 (* 1 = 0.0853077 loss)
I1125 16:09:14.274060 145709 sgd_solver.cpp:106] Iteration 2800, lr = 0.0001
I1125 16:10:28.217953 145709 solver.cpp:229] Iteration 2900, loss = 0.0758401
I1125 16:10:28.218088 145709 solver.cpp:245]     Train net output #0: loss = 0.0758394 (* 1 = 0.0758394 loss)
I1125 16:10:28.218099 145709 sgd_solver.cpp:106] Iteration 2900, lr = 0.0001
I1125 16:11:41.417361 145709 solver.cpp:338] Iteration 3000, Testing net (#0)
I1125 16:13:15.736490 145709 solver.cpp:406]     Test net output #0: accuracy = 0.617999
I1125 16:13:15.736677 145709 solver.cpp:406]     Test net output #1: loss = 3.59965 (* 1 = 3.59965 loss)
I1125 16:13:16.374912 145709 solver.cpp:229] Iteration 3000, loss = 0.0918672
I1125 16:13:16.374949 145709 solver.cpp:245]     Train net output #0: loss = 0.0918665 (* 1 = 0.0918665 loss)
I1125 16:13:16.374968 145709 sgd_solver.cpp:106] Iteration 3000, lr = 1e-05
I1125 16:14:30.336830 145709 solver.cpp:229] Iteration 3100, loss = 0.0666091
I1125 16:14:30.336968 145709 solver.cpp:245]     Train net output #0: loss = 0.0666084 (* 1 = 0.0666084 loss)
I1125 16:14:30.336982 145709 sgd_solver.cpp:106] Iteration 3100, lr = 1e-05
I1125 16:15:44.305037 145709 solver.cpp:229] Iteration 3200, loss = 0.0582512
I1125 16:15:44.305184 145709 solver.cpp:245]     Train net output #0: loss = 0.0582505 (* 1 = 0.0582505 loss)
I1125 16:15:44.305197 145709 sgd_solver.cpp:106] Iteration 3200, lr = 1e-05
I1125 16:16:58.274749 145709 solver.cpp:229] Iteration 3300, loss = 0.087295
I1125 16:16:58.274875 145709 solver.cpp:245]     Train net output #0: loss = 0.0872943 (* 1 = 0.0872943 loss)
I1125 16:16:58.274888 145709 sgd_solver.cpp:106] Iteration 3300, lr = 1e-05
I1125 16:18:12.213897 145709 solver.cpp:229] Iteration 3400, loss = 0.0609679
I1125 16:18:12.214030 145709 solver.cpp:245]     Train net output #0: loss = 0.0609672 (* 1 = 0.0609672 loss)
I1125 16:18:12.214042 145709 sgd_solver.cpp:106] Iteration 3400, lr = 1e-05
I1125 16:19:26.186702 145709 solver.cpp:229] Iteration 3500, loss = 0.0676332
I1125 16:19:26.186811 145709 solver.cpp:245]     Train net output #0: loss = 0.0676324 (* 1 = 0.0676324 loss)
I1125 16:19:26.186823 145709 sgd_solver.cpp:106] Iteration 3500, lr = 1e-05
I1125 16:20:40.178616 145709 solver.cpp:229] Iteration 3600, loss = 0.0619944
I1125 16:20:40.178813 145709 solver.cpp:245]     Train net output #0: loss = 0.0619937 (* 1 = 0.0619937 loss)
I1125 16:20:40.178838 145709 sgd_solver.cpp:106] Iteration 3600, lr = 1e-05
I1125 16:21:49.343283 145709 solver.cpp:229] Iteration 3700, loss = 0.0485382
I1125 16:21:49.343397 145709 solver.cpp:245]     Train net output #0: loss = 0.0485375 (* 1 = 0.0485375 loss)
I1125 16:21:49.343410 145709 sgd_solver.cpp:106] Iteration 3700, lr = 1e-05
I1125 16:22:58.207497 145709 solver.cpp:229] Iteration 3800, loss = 0.0669249
I1125 16:22:58.207737 145709 solver.cpp:245]     Train net output #0: loss = 0.0669242 (* 1 = 0.0669242 loss)
I1125 16:22:58.207762 145709 sgd_solver.cpp:106] Iteration 3800, lr = 1e-05
I1125 16:24:12.184877 145709 solver.cpp:229] Iteration 3900, loss = 0.076641
I1125 16:24:12.185012 145709 solver.cpp:245]     Train net output #0: loss = 0.0766403 (* 1 = 0.0766403 loss)
I1125 16:24:12.185024 145709 sgd_solver.cpp:106] Iteration 3900, lr = 1e-05
I1125 16:25:25.422773 145709 solver.cpp:456] Snapshotting to binary proto file caffenet_train_iter_4000.caffemodel
I1125 16:25:26.753970 145709 sgd_solver.cpp:273] Snapshotting solver state to binary proto file caffenet_train_iter_4000.solverstate
I1125 16:25:27.683960 145709 solver.cpp:338] Iteration 4000, Testing net (#0)
I1125 16:27:01.901969 145709 solver.cpp:406]     Test net output #0: accuracy = 0.615998
I1125 16:27:01.902148 145709 solver.cpp:406]     Test net output #1: loss = 3.61501 (* 1 = 3.61501 loss)
I1125 16:27:02.542991 145709 solver.cpp:229] Iteration 4000, loss = 0.083648
I1125 16:27:02.543030 145709 solver.cpp:245]     Train net output #0: loss = 0.0836472 (* 1 = 0.0836472 loss)
I1125 16:27:02.543045 145709 sgd_solver.cpp:106] Iteration 4000, lr = 1e-06
I1125 16:28:16.537745 145709 solver.cpp:229] Iteration 4100, loss = 0.0671084
I1125 16:28:16.537875 145709 solver.cpp:245]     Train net output #0: loss = 0.0671077 (* 1 = 0.0671077 loss)
I1125 16:28:16.537889 145709 sgd_solver.cpp:106] Iteration 4100, lr = 1e-06
I1125 16:29:30.553685 145709 solver.cpp:229] Iteration 4200, loss = 0.0637609
I1125 16:29:30.553827 145709 solver.cpp:245]     Train net output #0: loss = 0.0637602 (* 1 = 0.0637602 loss)
I1125 16:29:30.553838 145709 sgd_solver.cpp:106] Iteration 4200, lr = 1e-06
I1125 16:30:44.553475 145709 solver.cpp:229] Iteration 4300, loss = 0.108047
I1125 16:30:44.553606 145709 solver.cpp:245]     Train net output #0: loss = 0.108046 (* 1 = 0.108046 loss)
I1125 16:30:44.553618 145709 sgd_solver.cpp:106] Iteration 4300, lr = 1e-06
I1125 16:31:58.538502 145709 solver.cpp:229] Iteration 4400, loss = 0.0679677
I1125 16:31:58.538642 145709 solver.cpp:245]     Train net output #0: loss = 0.0679669 (* 1 = 0.0679669 loss)
I1125 16:31:58.538655 145709 sgd_solver.cpp:106] Iteration 4400, lr = 1e-06
I1125 16:33:12.534840 145709 solver.cpp:229] Iteration 4500, loss = 0.0794197
I1125 16:33:12.534970 145709 solver.cpp:245]     Train net output #0: loss = 0.0794189 (* 1 = 0.0794189 loss)
I1125 16:33:12.534982 145709 sgd_solver.cpp:106] Iteration 4500, lr = 1e-06
I1125 16:34:26.529525 145709 solver.cpp:229] Iteration 4600, loss = 0.0884199
I1125 16:34:26.529700 145709 solver.cpp:245]     Train net output #0: loss = 0.0884192 (* 1 = 0.0884192 loss)
I1125 16:34:26.529724 145709 sgd_solver.cpp:106] Iteration 4600, lr = 1e-06
I1125 16:35:33.364161 145709 solver.cpp:229] Iteration 4700, loss = 0.0267226
I1125 16:35:33.364326 145709 solver.cpp:245]     Train net output #0: loss = 0.0267218 (* 1 = 0.0267218 loss)
I1125 16:35:33.364353 145709 sgd_solver.cpp:106] Iteration 4700, lr = 1e-06
I1125 16:36:42.231523 145709 solver.cpp:229] Iteration 4800, loss = 0.0940246
I1125 16:36:42.231683 145709 solver.cpp:245]     Train net output #0: loss = 0.0940238 (* 1 = 0.0940238 loss)
I1125 16:36:42.231698 145709 sgd_solver.cpp:106] Iteration 4800, lr = 1e-06
I1125 16:37:56.195897 145709 solver.cpp:229] Iteration 4900, loss = 0.051262
I1125 16:37:56.196028 145709 solver.cpp:245]     Train net output #0: loss = 0.0512613 (* 1 = 0.0512613 loss)
I1125 16:37:56.196039 145709 sgd_solver.cpp:106] Iteration 4900, lr = 1e-06
I1125 16:39:09.402461 145709 solver.cpp:338] Iteration 5000, Testing net (#0)
I1125 16:40:43.729681 145709 solver.cpp:406]     Test net output #0: accuracy = 0.617999
I1125 16:40:43.729801 145709 solver.cpp:406]     Test net output #1: loss = 3.62397 (* 1 = 3.62397 loss)
I1125 16:40:44.367858 145709 solver.cpp:229] Iteration 5000, loss = 0.0943767
I1125 16:40:44.367890 145709 solver.cpp:245]     Train net output #0: loss = 0.0943759 (* 1 = 0.0943759 loss)
I1125 16:40:44.367905 145709 sgd_solver.cpp:106] Iteration 5000, lr = 1e-07
I1125 16:41:58.294586 145709 solver.cpp:229] Iteration 5100, loss = 0.0621558
I1125 16:41:58.294822 145709 solver.cpp:245]     Train net output #0: loss = 0.0621551 (* 1 = 0.0621551 loss)
I1125 16:41:58.294847 145709 sgd_solver.cpp:106] Iteration 5100, lr = 1e-07
I1125 16:43:12.247805 145709 solver.cpp:229] Iteration 5200, loss = 0.0436475
I1125 16:43:12.248003 145709 solver.cpp:245]     Train net output #0: loss = 0.0436468 (* 1 = 0.0436468 loss)
I1125 16:43:12.248028 145709 sgd_solver.cpp:106] Iteration 5200, lr = 1e-07
I1125 16:44:26.205651 145709 solver.cpp:229] Iteration 5300, loss = 0.0952511
I1125 16:44:26.205785 145709 solver.cpp:245]     Train net output #0: loss = 0.0952503 (* 1 = 0.0952503 loss)
I1125 16:44:26.205797 145709 sgd_solver.cpp:106] Iteration 5300, lr = 1e-07
I1125 16:45:40.163197 145709 solver.cpp:229] Iteration 5400, loss = 0.074565
I1125 16:45:40.163326 145709 solver.cpp:245]     Train net output #0: loss = 0.0745642 (* 1 = 0.0745642 loss)
I1125 16:45:40.163336 145709 sgd_solver.cpp:106] Iteration 5400, lr = 1e-07
I1125 16:46:54.110281 145709 solver.cpp:229] Iteration 5500, loss = 0.113398
I1125 16:46:54.110409 145709 solver.cpp:245]     Train net output #0: loss = 0.113398 (* 1 = 0.113398 loss)
I1125 16:46:54.110419 145709 sgd_solver.cpp:106] Iteration 5500, lr = 1e-07
I1125 16:48:08.068984 145709 solver.cpp:229] Iteration 5600, loss = 0.0779823
I1125 16:48:08.069119 145709 solver.cpp:245]     Train net output #0: loss = 0.0779816 (* 1 = 0.0779816 loss)
I1125 16:48:08.069130 145709 sgd_solver.cpp:106] Iteration 5600, lr = 1e-07
I1125 16:49:17.259802 145709 solver.cpp:229] Iteration 5700, loss = 0.0547732
I1125 16:49:17.259961 145709 solver.cpp:245]     Train net output #0: loss = 0.0547725 (* 1 = 0.0547725 loss)
I1125 16:49:17.259987 145709 sgd_solver.cpp:106] Iteration 5700, lr = 1e-07
I1125 16:50:26.114369 145709 solver.cpp:229] Iteration 5800, loss = 0.0766323
I1125 16:50:26.114503 145709 solver.cpp:245]     Train net output #0: loss = 0.0766315 (* 1 = 0.0766315 loss)
I1125 16:50:26.114514 145709 sgd_solver.cpp:106] Iteration 5800, lr = 1e-07
I1125 16:51:40.101959 145709 solver.cpp:229] Iteration 5900, loss = 0.0721422
I1125 16:51:40.102085 145709 solver.cpp:245]     Train net output #0: loss = 0.0721414 (* 1 = 0.0721414 loss)
I1125 16:51:40.102095 145709 sgd_solver.cpp:106] Iteration 5900, lr = 1e-07
I1125 16:52:53.392577 145709 solver.cpp:456] Snapshotting to binary proto file caffenet_train_iter_6000.caffemodel
I1125 16:52:54.626302 145709 sgd_solver.cpp:273] Snapshotting solver state to binary proto file caffenet_train_iter_6000.solverstate
I1125 16:52:55.524628 145709 solver.cpp:338] Iteration 6000, Testing net (#0)
I1125 16:54:29.765616 145709 solver.cpp:406]     Test net output #0: accuracy = 0.617999
I1125 16:54:29.765734 145709 solver.cpp:406]     Test net output #1: loss = 3.62434 (* 1 = 3.62434 loss)
I1125 16:54:30.403074 145709 solver.cpp:229] Iteration 6000, loss = 0.0958188
I1125 16:54:30.403105 145709 solver.cpp:245]     Train net output #0: loss = 0.095818 (* 1 = 0.095818 loss)
I1125 16:54:30.403117 145709 sgd_solver.cpp:106] Iteration 6000, lr = 1e-08
I1125 16:55:44.326396 145709 solver.cpp:229] Iteration 6100, loss = 0.0721503
I1125 16:55:44.326530 145709 solver.cpp:245]     Train net output #0: loss = 0.0721496 (* 1 = 0.0721496 loss)
I1125 16:55:44.326541 145709 sgd_solver.cpp:106] Iteration 6100, lr = 1e-08
I1125 16:56:58.326648 145709 solver.cpp:229] Iteration 6200, loss = 0.0491311
I1125 16:56:58.326845 145709 solver.cpp:245]     Train net output #0: loss = 0.0491304 (* 1 = 0.0491304 loss)
I1125 16:56:58.326869 145709 sgd_solver.cpp:106] Iteration 6200, lr = 1e-08
I1125 16:58:12.291889 145709 solver.cpp:229] Iteration 6300, loss = 0.0820409
I1125 16:58:12.292088 145709 solver.cpp:245]     Train net output #0: loss = 0.0820402 (* 1 = 0.0820402 loss)
I1125 16:58:12.292112 145709 sgd_solver.cpp:106] Iteration 6300, lr = 1e-08
I1125 16:59:26.258502 145709 solver.cpp:229] Iteration 6400, loss = 0.0768202
I1125 16:59:26.258682 145709 solver.cpp:245]     Train net output #0: loss = 0.0768195 (* 1 = 0.0768195 loss)
I1125 16:59:26.258697 145709 sgd_solver.cpp:106] Iteration 6400, lr = 1e-08
I1125 17:00:40.231653 145709 solver.cpp:229] Iteration 6500, loss = 0.105459
I1125 17:00:40.231853 145709 solver.cpp:245]     Train net output #0: loss = 0.105458 (* 1 = 0.105458 loss)
I1125 17:00:40.231878 145709 sgd_solver.cpp:106] Iteration 6500, lr = 1e-08
I1125 17:01:54.194269 145709 solver.cpp:229] Iteration 6600, loss = 0.0865536
I1125 17:01:54.194407 145709 solver.cpp:245]     Train net output #0: loss = 0.0865529 (* 1 = 0.0865529 loss)
I1125 17:01:54.194419 145709 sgd_solver.cpp:106] Iteration 6600, lr = 1e-08
I1125 17:03:01.010666 145709 solver.cpp:229] Iteration 6700, loss = 0.0639162
I1125 17:03:01.010804 145709 solver.cpp:245]     Train net output #0: loss = 0.0639154 (* 1 = 0.0639154 loss)
I1125 17:03:01.010815 145709 sgd_solver.cpp:106] Iteration 6700, lr = 1e-08
I1125 17:03:59.866617 145709 solver.cpp:229] Iteration 6800, loss = 0.100904
I1125 17:03:59.866843 145709 solver.cpp:245]     Train net output #0: loss = 0.100903 (* 1 = 0.100903 loss)
I1125 17:03:59.866876 145709 sgd_solver.cpp:106] Iteration 6800, lr = 1e-08
I1125 17:04:35.543763 145709 solver.cpp:229] Iteration 6900, loss = 0.0717219
I1125 17:04:35.543979 145709 solver.cpp:245]     Train net output #0: loss = 0.0717212 (* 1 = 0.0717212 loss)
I1125 17:04:35.544006 145709 sgd_solver.cpp:106] Iteration 6900, lr = 1e-08
I1125 17:05:10.862447 145709 solver.cpp:338] Iteration 7000, Testing net (#0)
I1125 17:05:51.396754 145709 solver.cpp:406]     Test net output #0: accuracy = 0.617999
I1125 17:05:51.396951 145709 solver.cpp:406]     Test net output #1: loss = 3.62443 (* 1 = 3.62443 loss)
I1125 17:05:51.703822 145709 solver.cpp:229] Iteration 7000, loss = 0.082928
I1125 17:05:51.703855 145709 solver.cpp:245]     Train net output #0: loss = 0.0829273 (* 1 = 0.0829273 loss)
I1125 17:05:51.703868 145709 sgd_solver.cpp:106] Iteration 7000, lr = 1e-09
I1125 17:06:27.385030 145709 solver.cpp:229] Iteration 7100, loss = 0.0791508
I1125 17:06:27.385252 145709 solver.cpp:245]     Train net output #0: loss = 0.07915 (* 1 = 0.07915 loss)
I1125 17:06:27.385287 145709 sgd_solver.cpp:106] Iteration 7100, lr = 1e-09
I1125 17:07:03.060082 145709 solver.cpp:229] Iteration 7200, loss = 0.0435103
I1125 17:07:03.060181 145709 solver.cpp:245]     Train net output #0: loss = 0.0435095 (* 1 = 0.0435095 loss)
I1125 17:07:03.060192 145709 sgd_solver.cpp:106] Iteration 7200, lr = 1e-09
I1125 17:07:38.740329 145709 solver.cpp:229] Iteration 7300, loss = 0.0859488
I1125 17:07:38.740473 145709 solver.cpp:245]     Train net output #0: loss = 0.085948 (* 1 = 0.085948 loss)
I1125 17:07:38.740484 145709 sgd_solver.cpp:106] Iteration 7300, lr = 1e-09
I1125 17:08:14.421741 145709 solver.cpp:229] Iteration 7400, loss = 0.0750438
I1125 17:08:14.421836 145709 solver.cpp:245]     Train net output #0: loss = 0.075043 (* 1 = 0.075043 loss)
I1125 17:08:14.421846 145709 sgd_solver.cpp:106] Iteration 7400, lr = 1e-09
I1125 17:08:50.100787 145709 solver.cpp:229] Iteration 7500, loss = 0.0670605
I1125 17:08:50.100919 145709 solver.cpp:245]     Train net output #0: loss = 0.0670598 (* 1 = 0.0670598 loss)
I1125 17:08:50.100930 145709 sgd_solver.cpp:106] Iteration 7500, lr = 1e-09
I1125 17:09:25.775228 145709 solver.cpp:229] Iteration 7600, loss = 0.0597159
I1125 17:09:25.775368 145709 solver.cpp:245]     Train net output #0: loss = 0.0597151 (* 1 = 0.0597151 loss)
I1125 17:09:25.775378 145709 sgd_solver.cpp:106] Iteration 7600, lr = 1e-09
I1125 17:10:01.453233 145709 solver.cpp:229] Iteration 7700, loss = 0.060479
I1125 17:10:01.453456 145709 solver.cpp:245]     Train net output #0: loss = 0.0604782 (* 1 = 0.0604782 loss)
I1125 17:10:01.453487 145709 sgd_solver.cpp:106] Iteration 7700, lr = 1e-09
I1125 17:10:37.135860 145709 solver.cpp:229] Iteration 7800, loss = 0.0841403
I1125 17:10:37.136090 145709 solver.cpp:245]     Train net output #0: loss = 0.0841395 (* 1 = 0.0841395 loss)
I1125 17:10:37.136116 145709 sgd_solver.cpp:106] Iteration 7800, lr = 1e-09
I1125 17:11:12.814833 145709 solver.cpp:229] Iteration 7900, loss = 0.0613688
I1125 17:11:12.815019 145709 solver.cpp:245]     Train net output #0: loss = 0.061368 (* 1 = 0.061368 loss)
I1125 17:11:12.815032 145709 sgd_solver.cpp:106] Iteration 7900, lr = 1e-09
I1125 17:11:48.137648 145709 solver.cpp:456] Snapshotting to binary proto file caffenet_train_iter_8000.caffemodel
I1125 17:11:49.412155 145709 sgd_solver.cpp:273] Snapshotting solver state to binary proto file caffenet_train_iter_8000.solverstate
I1125 17:11:50.438957 145709 solver.cpp:338] Iteration 8000, Testing net (#0)
I1125 17:12:30.926059 145709 solver.cpp:406]     Test net output #0: accuracy = 0.617999
I1125 17:12:30.926185 145709 solver.cpp:406]     Test net output #1: loss = 3.62443 (* 1 = 3.62443 loss)
I1125 17:12:31.233897 145709 solver.cpp:229] Iteration 8000, loss = 0.0786585
I1125 17:12:31.233930 145709 solver.cpp:245]     Train net output #0: loss = 0.0786578 (* 1 = 0.0786578 loss)
I1125 17:12:31.233945 145709 sgd_solver.cpp:106] Iteration 8000, lr = 1e-10
I1125 17:13:06.907519 145709 solver.cpp:229] Iteration 8100, loss = 0.0586752
I1125 17:13:06.907744 145709 solver.cpp:245]     Train net output #0: loss = 0.0586744 (* 1 = 0.0586744 loss)
I1125 17:13:06.907778 145709 sgd_solver.cpp:106] Iteration 8100, lr = 1e-10
I1125 17:13:42.579380 145709 solver.cpp:229] Iteration 8200, loss = 0.048903
I1125 17:13:42.579524 145709 solver.cpp:245]     Train net output #0: loss = 0.0489023 (* 1 = 0.0489023 loss)
I1125 17:13:42.579535 145709 sgd_solver.cpp:106] Iteration 8200, lr = 1e-10
I1125 17:14:18.258230 145709 solver.cpp:229] Iteration 8300, loss = 0.0781402
I1125 17:14:18.258448 145709 solver.cpp:245]     Train net output #0: loss = 0.0781394 (* 1 = 0.0781394 loss)
I1125 17:14:18.258482 145709 sgd_solver.cpp:106] Iteration 8300, lr = 1e-10
I1125 17:14:53.934701 145709 solver.cpp:229] Iteration 8400, loss = 0.0692972
I1125 17:14:53.934926 145709 solver.cpp:245]     Train net output #0: loss = 0.0692964 (* 1 = 0.0692964 loss)
I1125 17:14:53.934952 145709 sgd_solver.cpp:106] Iteration 8400, lr = 1e-10
I1125 17:15:29.617110 145709 solver.cpp:229] Iteration 8500, loss = 0.0966348
I1125 17:15:29.617249 145709 solver.cpp:245]     Train net output #0: loss = 0.096634 (* 1 = 0.096634 loss)
I1125 17:15:29.617259 145709 sgd_solver.cpp:106] Iteration 8500, lr = 1e-10
I1125 17:16:05.302835 145709 solver.cpp:229] Iteration 8600, loss = 0.0699078
I1125 17:16:05.303059 145709 solver.cpp:245]     Train net output #0: loss = 0.069907 (* 1 = 0.069907 loss)
I1125 17:16:05.303094 145709 sgd_solver.cpp:106] Iteration 8600, lr = 1e-10
I1125 17:16:40.983085 145709 solver.cpp:229] Iteration 8700, loss = 0.0582365
I1125 17:16:40.983299 145709 solver.cpp:245]     Train net output #0: loss = 0.0582358 (* 1 = 0.0582358 loss)
I1125 17:16:40.983326 145709 sgd_solver.cpp:106] Iteration 8700, lr = 1e-10
I1125 17:17:16.658993 145709 solver.cpp:229] Iteration 8800, loss = 0.0888748
I1125 17:17:16.659148 145709 solver.cpp:245]     Train net output #0: loss = 0.088874 (* 1 = 0.088874 loss)
I1125 17:17:16.659159 145709 sgd_solver.cpp:106] Iteration 8800, lr = 1e-10
I1125 17:17:52.338677 145709 solver.cpp:229] Iteration 8900, loss = 0.0707792
I1125 17:17:52.338908 145709 solver.cpp:245]     Train net output #0: loss = 0.0707785 (* 1 = 0.0707785 loss)
I1125 17:17:52.338935 145709 sgd_solver.cpp:106] Iteration 8900, lr = 1e-10
I1125 17:18:27.662449 145709 solver.cpp:338] Iteration 9000, Testing net (#0)
I1125 17:19:08.198379 145709 solver.cpp:406]     Test net output #0: accuracy = 0.617999
I1125 17:19:08.198536 145709 solver.cpp:406]     Test net output #1: loss = 3.62443 (* 1 = 3.62443 loss)
I1125 17:19:08.505553 145709 solver.cpp:229] Iteration 9000, loss = 0.108487
I1125 17:19:08.505591 145709 solver.cpp:245]     Train net output #0: loss = 0.108487 (* 1 = 0.108487 loss)
I1125 17:19:08.505607 145709 sgd_solver.cpp:106] Iteration 9000, lr = 1e-11
I1125 17:19:44.190778 145709 solver.cpp:229] Iteration 9100, loss = 0.074096
I1125 17:19:44.190928 145709 solver.cpp:245]     Train net output #0: loss = 0.0740953 (* 1 = 0.0740953 loss)
I1125 17:19:44.190940 145709 sgd_solver.cpp:106] Iteration 9100, lr = 1e-11
I1125 17:20:19.881901 145709 solver.cpp:229] Iteration 9200, loss = 0.0406498
I1125 17:20:19.882148 145709 solver.cpp:245]     Train net output #0: loss = 0.0406491 (* 1 = 0.0406491 loss)
I1125 17:20:19.882174 145709 sgd_solver.cpp:106] Iteration 9200, lr = 1e-11
I1125 17:20:55.569738 145709 solver.cpp:229] Iteration 9300, loss = 0.0997767
I1125 17:20:55.569975 145709 solver.cpp:245]     Train net output #0: loss = 0.099776 (* 1 = 0.099776 loss)
I1125 17:20:55.570003 145709 sgd_solver.cpp:106] Iteration 9300, lr = 1e-11
I1125 17:21:31.256419 145709 solver.cpp:229] Iteration 9400, loss = 0.0852992
I1125 17:21:31.256659 145709 solver.cpp:245]     Train net output #0: loss = 0.0852985 (* 1 = 0.0852985 loss)
I1125 17:21:31.256685 145709 sgd_solver.cpp:106] Iteration 9400, lr = 1e-11
I1125 17:22:06.939687 145709 solver.cpp:229] Iteration 9500, loss = 0.0911299
I1125 17:22:06.939836 145709 solver.cpp:245]     Train net output #0: loss = 0.0911292 (* 1 = 0.0911292 loss)
I1125 17:22:06.939848 145709 sgd_solver.cpp:106] Iteration 9500, lr = 1e-11
I1125 17:22:42.623687 145709 solver.cpp:229] Iteration 9600, loss = 0.0584725
I1125 17:22:42.623819 145709 solver.cpp:245]     Train net output #0: loss = 0.0584717 (* 1 = 0.0584717 loss)
I1125 17:22:42.623831 145709 sgd_solver.cpp:106] Iteration 9600, lr = 1e-11
I1125 17:23:18.303361 145709 solver.cpp:229] Iteration 9700, loss = 0.0425348
I1125 17:23:18.303550 145709 solver.cpp:245]     Train net output #0: loss = 0.042534 (* 1 = 0.042534 loss)
I1125 17:23:18.303588 145709 sgd_solver.cpp:106] Iteration 9700, lr = 1e-11
I1125 17:23:53.991528 145709 solver.cpp:229] Iteration 9800, loss = 0.0731262
I1125 17:23:53.991725 145709 solver.cpp:245]     Train net output #0: loss = 0.0731255 (* 1 = 0.0731255 loss)
I1125 17:23:53.991750 145709 sgd_solver.cpp:106] Iteration 9800, lr = 1e-11
I1125 17:24:29.679020 145709 solver.cpp:229] Iteration 9900, loss = 0.083942
I1125 17:24:29.679244 145709 solver.cpp:245]     Train net output #0: loss = 0.0839413 (* 1 = 0.0839413 loss)
I1125 17:24:29.679280 145709 sgd_solver.cpp:106] Iteration 9900, lr = 1e-11
I1125 17:25:05.009902 145709 solver.cpp:456] Snapshotting to binary proto file caffenet_train_iter_10000.caffemodel
I1125 17:25:06.264654 145709 sgd_solver.cpp:273] Snapshotting solver state to binary proto file caffenet_train_iter_10000.solverstate
I1125 17:25:07.465361 145709 solver.cpp:318] Iteration 10000, loss = 0.114594
I1125 17:25:07.465409 145709 solver.cpp:338] Iteration 10000, Testing net (#0)
I1125 17:25:47.955487 145709 solver.cpp:406]     Test net output #0: accuracy = 0.617999
I1125 17:25:47.955672 145709 solver.cpp:406]     Test net output #1: loss = 3.62443 (* 1 = 3.62443 loss)
I1125 17:25:47.955690 145709 solver.cpp:323] Optimization Done.
I1125 17:25:47.955695 145709 caffe.cpp:222] Optimization Done.
